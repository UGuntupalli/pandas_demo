{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: https://pandas.pydata.org/pandas-docs/stable/index.html\n",
    "\n",
    "### What is Pandas? \n",
    "Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "### What data structures does Pandas use?\n",
    "Pandas has 2 primary data structures - namely Series(1-D) and DataFrame(2-D). \n",
    "\n",
    "### What to expect from this tutorial/notebook? \n",
    "This tutorial is intended to introduce you to Pandas from a high level and then expose you to\n",
    "- Data Acquisition \n",
    "- Data Cleaning \n",
    "- Data Filtering \n",
    "- Data Aggregation \n",
    "- Data Analysis (depending on time availability)\n",
    "\n",
    "### How is this different from the countless other materials that are publicly available? \n",
    "It is by no means exhaustive or extensive, rather you can consider it my share of learnings that I picked up and learned as I attempted to use Python. I will be sharing tips and tricks that I found to be helpful, but if you know something better, you are welcome to share it with me/us. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create data-structures in Pandas ? "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "import pickle  \n",
    "import csv \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    2\n1    3\n2    5\n3    7\nName: Primes_Under_10, dtype: int64\n0          DesertPy\n1      SoCal_Python\n2    PyLadies_of_LA\nName: Some_Python_Meetups, dtype: object\n0    2\n1    a\n2    4\n3    b\nName: Mixed_Series, dtype: object\n"
    }
   ],
   "source": [
    "# Creating a series \n",
    "my_numeric_series = pd.Series([2, 3, 5, 7], name=\"Primes_Under_10\")\n",
    "print(my_numeric_series)\n",
    "my_character_series = pd.Series([\"DesertPy\", \"SoCal_Python\", \"PyLadies_of_LA\"], name=\"Some_Python_Meetups\")\n",
    "print(my_character_series)\n",
    "my_mixed_series = pd.Series([2, \"a\", 4, \"b\"], name=\"Mixed_Series\")\n",
    "print(my_mixed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "******************************\nPrinting Dataframe from method 1\n******************************\n           Name       State  Term_Expiry\n0    Doug Ducey     Arizona         2023\n1  Gavin Newsom  California         2023\n2  Ron Desantis     Florida         2023\n3  Andrew Cuomo    New York         2022\n4    Brian Kemp     Georgia         2023\n******************************\nPrinting Dataframe from method 2\n******************************\n           Name           State  Term_Expiry\n0    Jay Inslee      Washington         2021\n1    Ned Lamont     Connecticut         2023\n2  Andy Beshear        Kentucky         2023\n3    Roy Cooper  North Carolina         2021\n******************************\nPrinting Dataframe from method 3\n******************************\n             USA  Brazil  Canada\nState_Count   50      26      10\n******************************\nPrinting Dataframe from method 4\n******************************\n  Stock_Symobl  Dream_Price\n0         AAPL           50\n1         AMZN           10\n2            V            1\n3           MA           78\n******************************\nPrinting Dataframe from method 5\n******************************\n         Place_I_Wanted_To_Be Place_I_Am_At\nJanuary           New Zealand          Home\nFebruary                 Fiji          Home\nMarch                 Bahamas          Home\n"
    }
   ],
   "source": [
    "# Creating a data frame \n",
    "# Method 1 - from list of lists \n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from method 1\")\n",
    "print(\"******************************\")\n",
    "\n",
    "list_of_lists = [[\"Doug Ducey\", \"Arizona\", 2023], [\"Gavin Newsom\", \"California\", 2023], \n",
    "                 [\"Ron Desantis\", \"Florida\", 2023], [\"Andrew Cuomo\", \"New York\", 2022],\n",
    "                 [\"Brian Kemp\", \"Georgia\", 2023]]\n",
    "governors_in_the_news_df = pd.DataFrame(data=list_of_lists, columns=[\"Name\", \"State\", \"Term_Expiry\"])\n",
    "print(governors_in_the_news_df)\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from method 2\")\n",
    "print(\"******************************\")\n",
    "\n",
    "# Method 2 - from dictionary of lists\n",
    "dict_of_lists = {\"Name\": [\"Jay Inslee\", \"Ned Lamont\", \"Andy Beshear\", \"Roy Cooper\"],\n",
    "                \"State\": [\"Washington\", \"Connecticut\", \"Kentucky\", \"North Carolina\"],\n",
    "                \"Term_Expiry\": [2021, 2023, 2023, 2021]}\n",
    "governors_df = pd.DataFrame(data=dict_of_lists)\n",
    "print(governors_df)\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from method 3\")\n",
    "print(\"******************************\")\n",
    "\n",
    "# Method 3 - from list of dictionaries \n",
    "list_of_dicts = [{'USA': 50, 'Brazil': 26, 'Canada':10}]\n",
    "states_in_countries_df = pd.DataFrame(data=list_of_dicts, index=[\"State_Count\"])\n",
    "print(states_in_countries_df)\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from method 4\")\n",
    "print(\"******************************\")\n",
    "\n",
    "# Method 4 - from lists with zip \n",
    "stock_symbols = [\"AAPL\", \"AMZN\", \"V\", \"MA\"]\n",
    "prices_i_wish_i_bought_them_at = [50, 10, 1, 78]\n",
    "stocks_i_wanted_df = pd.DataFrame(data=list(zip(stock_symbols, prices_i_wish_i_bought_them_at)),\n",
    "                                  columns=[\"Stock_Symobl\", \"Dream_Price\"]) \n",
    "print(stocks_i_wanted_df)\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from method 5\")\n",
    "print(\"******************************\")\n",
    "\n",
    "# Method 5 - dict of pd.Series \n",
    "dict_of_series = {'Place_I_Wanted_To_Be' : \n",
    "                    pd.Series([\"New Zealand\", \"Fiji\", \"Bahamas\"], index =[\"January\",    \"February\", \"March\"]),                  'Place_I_Am_At' : \n",
    "                    pd.Series([\"Home\", \"Home\", \"Home\"], index =[\"January\", \"February\", \"March\"])} \n",
    "lockdown_mood_df = pd.DataFrame(dict_of_series)\n",
    "print(lockdown_mood_df)\n",
    "\n",
    "# Delete the temporary varibles and datasets to avoid cluttering of workspace - these will not be used below\n",
    "del my_numeric_series, my_character_series, my_mixed_series, list_of_lists, governors_in_the_news_df, dict_of_lists\n",
    "del governors_df, list_of_dicts, states_in_countries_df, stock_symbols, prices_i_wish_i_bought_them_at, stocks_i_wanted_df, dict_of_series, lockdown_mood_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways-1\n",
    "#### From the above examples, it is helpful to identify a few takeaways: \n",
    "1. Series and DataFrame can represent most of the commonly used data sets. Constructing your data into a Series or a DataFrame allows you to leverage a lot of built-in functionality that Pandas offers \n",
    "2. Series and DataFrame support homogeneous and heterogeneous data - meaning they can handle same data types as well as different data types \n",
    "3. Series and DataFrame have an index property which defaults to an integer but can be set as desired (imagine time stamps, letters etc.)\n",
    "4. Pandas 1.0.0 deprecated the testing module and limited to only assertion functions. While not advisable, if you are using a version < 1.0.0, pandas.util.testing offers close to 30 different built-in functions to whip up different data frames that make it easy to test. You can get the list of possible functions like so \n",
    "\n",
    "```\n",
    "import pandas.util.testing as tm \n",
    "dataframe_constructor_functions = [i for i in dir(tm) if i.startswith('make')]\n",
    "print(dataframe_constructor_functions)\n",
    "```\n",
    "\n",
    "5. While all of these are good to know, a typical use-case would not require a user to create data, rather import/acquire data from several different data sources - which leads us to our first topic of Data Acquisition "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition \n",
    "\n",
    "#### One of the most powerful and appealing aspects of Pandas is its ability to easily acquire and ingest data from several different data sources including but not limited to: \n",
    "- CSV\n",
    "- Text \n",
    "- JSON \n",
    "- HTML \n",
    "- Excel\n",
    "- SQL\n",
    "\n",
    "  An exhaustive list can be found here - https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "******************************\nPrinting Dataframe from CSV\n******************************\n  Province/State Country/Region      Lat     Long  1/22/20  1/23/20  1/24/20  \\\n0            NaN    Afghanistan  33.0000  65.0000        0        0        0   \n1            NaN        Albania  41.1533  20.1683        0        0        0   \n2            NaN        Algeria  28.0339   1.6596        0        0        0   \n3            NaN        Andorra  42.5063   1.5218        0        0        0   \n4            NaN         Angola -11.2027  17.8739        0        0        0   \n\n   1/25/20  1/26/20  1/27/20  ...  3/24/20  3/25/20  3/26/20  3/27/20  \\\n0        0        0        0  ...       74       84       94      110   \n1        0        0        0  ...      123      146      174      186   \n2        0        0        0  ...      264      302      367      409   \n3        0        0        0  ...      164      188      224      267   \n4        0        0        0  ...        3        3        4        4   \n\n   3/28/20  3/29/20  3/30/20  3/31/20  4/1/20  4/2/20  \n0      110      120      170      174     237     273  \n1      197      212      223      243     259     277  \n2      454      511      584      716     847     986  \n3      308      334      370      376     390     428  \n4        5        7        7        7       8       8  \n\n[5 rows x 76 columns]\n******************************\nPrinting Dataframe from JSON\n******************************\n  city  trips       date  value  price request_date   medium  %price   type  \\\n0   ab      4 2014-01-25    4.7    1.1   2014-06-17   iPhone    15.4   True   \n1   bc      0 2014-01-29    5.0    1.0   2014-05-05  Android     0.0  False   \n\n   Weekly_pct  avg_dist  avg_price  weekly_pct  \n0        46.2      3.67          5         NaN  \n1         NaN      8.26          5        50.0  \n******************************\nPrinting Dataframe from Excel\n******************************\n"
    },
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a379d359f352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# excel_file_path = os.path.join(current_directory, \"covid-19_data\",\"time_series_covid19_confirmed_recovered.xlsx\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"covid-19_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mrecovered_global_cases_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time_series_covid19_recovered_global.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecovered_global_cases_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas_demo\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas_demo\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas_demo\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas_demo\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "# CSV \n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from CSV\")\n",
    "print(\"******************************\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "confimed_global_cases_file_path = os.path.join(current_directory, \"covid-19_data\",\"time_series_covid19_confirmed_global.csv\") \n",
    "confirmed_global_cases_df = pd.read_csv(filepath_or_buffer=confimed_global_cases_file_path) \n",
    "print(confirmed_global_cases_df.head()) \n",
    "\n",
    "# JSON \n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from JSON\")\n",
    "print(\"******************************\")\n",
    "json_file_path = os.path.join(current_directory, \"sample_json.json\")\n",
    "json_df = pd.read_json(json_file_path)\n",
    "print(json_df)\n",
    "\n",
    "# Excel\n",
    "print(\"******************************\")\n",
    "print(\"Printing Dataframe from Excel\")\n",
    "print(\"******************************\")\n",
    "# excel_file_path = os.path.join(current_directory, \"covid-19_data\",\"time_series_covid19_confirmed_recovered.xlsx\")\n",
    "os.chdir(\"covid-19_data\")\n",
    "recovered_global_cases_df = pd.read_excel(\"time_series_covid19_recovered_global.xlsx\")\n",
    "os.chdir(current_directory)\n",
    "print(recovered_global_cases_df.head())\n",
    "\n",
    "# Delete the temporary varibles and datasets to avoid cluttering of workspace - these will not be used below\n",
    "del json_file_path, json_df, recovered_global_cases_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways-2\n",
    "\n",
    "#### Based on the few limited examples above: \n",
    "1. Pandas has several robust i/o parsers which makes it really easy to consume data from several different sources \n",
    "2. If you are going to work with pandas, it is best to use pandas to acquire the data, as long as parser exists because they are optimized to handle large sets of data \n",
    "3. What would have been a great example would be to consume SQL data, but since I don't have enough power on my machine to install a SQL Server program, I am forced to skip that - if you have access to a database, do try and consume data from the database. You would be needing either pyodbc or sqlalchemy or a similar package as a wrapper. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning  & Data Filtering (They are quite intertwined)\n",
    "\n",
    "#### Messy (or) Unorganized data is very common. Even if the data is organized and logically makes sense, it might have missing data or NaN's or NA's  or may not be organized in a way the user wants it, which could be a hindrance to smooth analysis of your data. \n",
    "\n",
    "### Tip #1 - If you are going to modify a data frame and want the original data, create a copy "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By copying a dataframe with deep flag turned on, a new data frame is created including a copy of the data and the indices. \n",
    "# Changes made to this new data frame are not reflected in the original data frame object and vice-versa \n",
    "confirmed_global_cases_copy_df = confirmed_global_cases_df.copy(deep=True)\n",
    "\n",
    "# Let us we want to only look at countries where there is state/province-level data available \n",
    "confirmed_global_cases_copy_df = confirmed_global_cases_copy_df[confirmed_global_cases_copy_df[\"Province/State\"].notnull()] # Method 1 of filtering data in dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the data frame index does not hold any special order now and it does not convey any meaning by itself, unless we reference it or compare with the original data frame. We can fix that by re-setting the index \n",
    "confirmed_global_cases_copy_df = confirmed_global_cases_copy_df.reset_index(drop=True) \n",
    "# drop flag prevents the column from being added back to the dataframe as a new column. also remember to assign the data frame back to your variable. resetting of index, returns an object and unless we capture it and re-assign to the same variable, the change is lost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us say, we only want countries in the Northern Hemisphere - please note that this is just one method to create a new column. I want to show as many different options as possible \n",
    "def hemisphere_flag(df):\n",
    "    if (df[\"Lat\"] >= 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "confirmed_global_cases_copy_df[\"Northern_Hemisphere_Flag\"] = confirmed_global_cases_copy_df.apply(hemisphere_flag, axis=1)\n",
    "northern_hemisphere_confirmed_cases_df = confirmed_global_cases_copy_df.query(\"Northern_Hemisphere_Flag == 1\") # Method 2 of filtering data in dataframe \n",
    "northern_hemisphere_confirmed_cases_df = northern_hemisphere_confirmed_cases_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how we can subset the data frame based on multiple conditions \n",
    "hubei_data_df = northern_hemisphere_confirmed_cases_df.loc[(northern_hemisphere_confirmed_cases_df[\"Country/Region\"]==\"China\") & (northern_hemisphere_confirmed_cases_df[\"Province/State\"]==\"Hubei\")] # Method 3 of filtering data in dataframe \n",
    "\n",
    "# Alternatively , if you wanted to generate a flag and not necessarily subset, you can use Numpy as follows \n",
    "northern_hemisphere_confirmed_cases_df['Alberta_Flag'] = np.where((northern_hemisphere_confirmed_cases_df[\"Country/Region\"]==\"Canada\") & (northern_hemisphere_confirmed_cases_df[\"Province/State\"]==\"Alberta\"), 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the hubei data set, since we know that Hubei is in China and China is in the Northern Hemisphere, let us try to drop/delete the columns from the data set \n",
    "columns_to_drop_in_hubei_data = [\"Country/Region\", \"Northern_Hemisphere_Flag\"]\n",
    "hubei_data_df.drop(columns_to_drop_in_hubei_data, inplace=True, axis=1)\n",
    "\n",
    "# Alternatively, we could do the following, let us perform a similar action on Northern Hemisphere data \n",
    "columns_to_drop_in_northern_hemisphere = [\"Alberta_Flag\", \"Northern_Hemisphere_Flag\"]\n",
    "northern_hemisphere_confirmed_cases_df.drop(columns=columns_to_drop_in_northern_hemisphere, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 80 entries, 0 to 79\nData columns (total 77 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Province/State            80 non-null     object \n 1   Country/Region            80 non-null     object \n 2   Lat                       80 non-null     float64\n 3   Long                      80 non-null     float64\n 4   1/22/20                   80 non-null     int64  \n 5   1/23/20                   80 non-null     int64  \n 6   1/24/20                   80 non-null     int64  \n 7   1/25/20                   80 non-null     int64  \n 8   1/26/20                   80 non-null     int64  \n 9   1/27/20                   80 non-null     int64  \n 10  1/28/20                   80 non-null     int64  \n 11  1/29/20                   80 non-null     int64  \n 12  1/30/20                   80 non-null     int64  \n 13  1/31/20                   80 non-null     int64  \n 14  2/1/20                    80 non-null     int64  \n 15  2/2/20                    80 non-null     int64  \n 16  2/3/20                    80 non-null     int64  \n 17  2/4/20                    80 non-null     int64  \n 18  2/5/20                    80 non-null     int64  \n 19  2/6/20                    80 non-null     int64  \n 20  2/7/20                    80 non-null     int64  \n 21  2/8/20                    80 non-null     int64  \n 22  2/9/20                    80 non-null     int64  \n 23  2/10/20                   80 non-null     int64  \n 24  2/11/20                   80 non-null     int64  \n 25  2/12/20                   80 non-null     int64  \n 26  2/13/20                   80 non-null     int64  \n 27  2/14/20                   80 non-null     int64  \n 28  2/15/20                   80 non-null     int64  \n 29  2/16/20                   80 non-null     int64  \n 30  2/17/20                   80 non-null     int64  \n 31  2/18/20                   80 non-null     int64  \n 32  2/19/20                   80 non-null     int64  \n 33  2/20/20                   80 non-null     int64  \n 34  2/21/20                   80 non-null     int64  \n 35  2/22/20                   80 non-null     int64  \n 36  2/23/20                   80 non-null     int64  \n 37  2/24/20                   80 non-null     int64  \n 38  2/25/20                   80 non-null     int64  \n 39  2/26/20                   80 non-null     int64  \n 40  2/27/20                   80 non-null     int64  \n 41  2/28/20                   80 non-null     int64  \n 42  2/29/20                   80 non-null     int64  \n 43  3/1/20                    80 non-null     int64  \n 44  3/2/20                    80 non-null     int64  \n 45  3/3/20                    80 non-null     int64  \n 46  3/4/20                    80 non-null     int64  \n 47  3/5/20                    80 non-null     int64  \n 48  3/6/20                    80 non-null     int64  \n 49  3/7/20                    80 non-null     int64  \n 50  3/8/20                    80 non-null     int64  \n 51  3/9/20                    80 non-null     int64  \n 52  3/10/20                   80 non-null     int64  \n 53  3/11/20                   80 non-null     int64  \n 54  3/12/20                   80 non-null     int64  \n 55  3/13/20                   80 non-null     int64  \n 56  3/14/20                   80 non-null     int64  \n 57  3/15/20                   80 non-null     int64  \n 58  3/16/20                   80 non-null     int64  \n 59  3/17/20                   80 non-null     int64  \n 60  3/18/20                   80 non-null     int64  \n 61  3/19/20                   80 non-null     int64  \n 62  3/20/20                   80 non-null     int64  \n 63  3/21/20                   80 non-null     int64  \n 64  3/22/20                   80 non-null     int64  \n 65  3/23/20                   80 non-null     int64  \n 66  3/24/20                   80 non-null     int64  \n 67  3/25/20                   80 non-null     int64  \n 68  3/26/20                   80 non-null     int64  \n 69  3/27/20                   80 non-null     int64  \n 70  3/28/20                   80 non-null     int64  \n 71  3/29/20                   80 non-null     int64  \n 72  3/30/20                   80 non-null     int64  \n 73  3/31/20                   80 non-null     int64  \n 74  4/1/20                    80 non-null     int64  \n 75  4/2/20                    80 non-null     int64  \n 76  Northern_Hemisphere_Flag  80 non-null     int64  \ndtypes: float64(2), int64(73), object(2)\nmemory usage: 48.2+ KB\n"
    }
   ],
   "source": [
    "# Let us delete a couple of the intermediate data frames that we created for demonstration purposes\n",
    "del hubei_data_df, northern_hemisphere_confirmed_cases_df\n",
    "\n",
    "# Now, let us use the copy we created to learn something else \n",
    "confirmed_global_cases_copy_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Province/State              object\nCountry/Region              object\nLat                         object\nLong                        object\n1/22/20                      int64\n                             ...  \n3/30/20                      int64\n3/31/20                      int64\n4/1/20                       int64\n4/2/20                       int64\nNorthern_Hemisphere_Flag     int64\nLength: 77, dtype: object"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Changing the dtype of a column is another handy data cleansing technique that you would be using a lot \n",
    "confirmed_global_cases_copy_df.astype({'Lat': 'object', 'Long': 'object'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Province/State              object\nCountry/Region              object\nLat                         object\nLong                        object\n1/22/20                     object\n                             ...  \n3/30/20                     object\n3/31/20                     object\n4/1/20                      object\n4/2/20                      object\nNorthern_Hemisphere_Flag    object\nLength: 77, dtype: object"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# If it instead made sense to case all your columns to a single data type you could simply do this\n",
    "confirmed_global_cases_copy_df.astype('object').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let us look at \"index\", a very powerful and handy component of a Pandas data frame \n",
    "# While it does not need to be unique like a SQL primary key, it will definitely help optimize execution of a lot of the methods if the index is unique. \n",
    "china_confirmed_cases_df = confirmed_global_cases_copy_df[confirmed_global_cases_copy_df[\"Country/Region\"].isin([\"China\"])]  \n",
    "# We can see that and we have covered this before that index has been disrupted after the subset, so let us create a new index \n",
    "china_confirmed_cases_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let us assign one of the columns as the index \n",
    "china_confirmed_cases_df.set_index(keys=[\"Province/State\"], drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n       'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n       'Hong Kong', 'Hubei', 'Hunan', 'Inner Mongolia', 'Jiangsu', 'Jiangxi',\n       'Jilin', 'Liaoning', 'Macau', 'Ningxia', 'Qinghai', 'Shaanxi',\n       'Shandong', 'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet',\n       'Xinjiang', 'Yunnan', 'Zhejiang'],\n      dtype='object', name='Province/State')"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "china_confirmed_cases_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so how does setting the index really help ? \n",
    "china_subset_df = china_confirmed_cases_df.filter(like=\"Hubei\", axis=0) # It allows filtering based on the unique index - especially powerful if data is time-series based\n",
    "# Method 4 of filtering data in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways-3\n",
    "\n",
    "#### We covered several data cleaning and data filtering techniques, key points to remeber are: \n",
    "1. Unless, we  want to modify the source data, it is a good idea to make a deep copy of a data frame before making any modifications \n",
    "2. There are several different options to create a new column in a data frame, we covered a couple of these\n",
    "      - Using the apply function on a user defined function \n",
    "      - Using the np.where construct \n",
    "3. We also looked at different ways to drop unwanted columns in a given dataframe (users are welcome to choose the construct that makes most sense to them)\n",
    "      - Using the axis keyword argument\n",
    "      - Using the columns keyword argument \n",
    "4. Using the info method with verbose set to true, produces a lot of helpful \n",
    "5. Casting individual column(s) to the desired data type as well as entire data frame to a data type of choice is possible using the astype() \n",
    "6. Resetting, setting and filtering using index of a dataframe (what has not been covered here is MultiIndex. That is a very useful concept, nut I am still trying to get a handle and could not cover that today)\n",
    "7. Different ways to filter data in a dataframe:\n",
    "      - Using built in methods like ```pd.DataFrame.notnull()``` or ```pd.DataFrame.isin()``` along with logical indexing \n",
    "      - Using ``` pd.DataFrame.query() ```  \n",
    "      - Using boolean indexing (Method 1 can be considered a subset of this)\n",
    "      - Using ``` pd.DataFrame.filter() ```\n",
    "      - There are other methods which haven't been covered like ```pd.DataFrame.iloc()``` which is better suited when the indices of rows desired are known"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Aggregation \n",
    "\n",
    "#### Source: https://data.open-power-system-data.org/household_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Time Series Household data from open power system platform \n",
    "os.chdir(current_directory)\n",
    "household_data_df = pd.read_csv(\"household_data_15min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data_df.drop(columns=[\"cet_cest_timestamp\"], inplace=True) # Dropping the localized timestamp column as we are not going to try and deep dive into that \n",
    "household_data_df[\"utc_timestamp\"] = pd.to_datetime(household_data_df[\"utc_timestamp\"], format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "household_data_df.set_index(\"utc_timestamp\", drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     DE_KN_industrial1_grid_import  DE_KN_industrial1_pv_1  \\\nutc_timestamp                                                                \n2017-01-01 00:00:00                     261301.539                5390.673   \n2017-01-01 00:15:00                     261303.961                5390.673   \n2017-01-01 00:30:00                     261305.914                5390.673   \n2017-01-01 00:45:00                     261309.914                5390.673   \n2017-01-01 01:00:00                     261312.602                5390.673   \n...                                            ...                     ...   \n2019-05-01 21:00:00                            NaN                     NaN   \n2019-05-01 21:15:00                            NaN                     NaN   \n2019-05-01 21:30:00                            NaN                     NaN   \n2019-05-01 21:45:00                            NaN                     NaN   \n2019-05-01 22:00:00                            NaN                     NaN   \n\n                     DE_KN_industrial1_pv_2  DE_KN_industrial2_grid_import  \\\nutc_timestamp                                                                \n2017-01-01 00:00:00                4253.803                      10716.058   \n2017-01-01 00:15:00                4253.803                      10716.449   \n2017-01-01 00:30:00                4253.803                      10716.865   \n2017-01-01 00:45:00                4253.803                      10717.264   \n2017-01-01 01:00:00                4253.803                      10717.680   \n...                                     ...                            ...   \n2019-05-01 21:00:00                     NaN                            NaN   \n2019-05-01 21:15:00                     NaN                            NaN   \n2019-05-01 21:30:00                     NaN                            NaN   \n2019-05-01 21:45:00                     NaN                            NaN   \n2019-05-01 22:00:00                     NaN                            NaN   \n\n                     DE_KN_industrial2_pv  DE_KN_industrial2_storage_charge  \\\nutc_timestamp                                                                 \n2017-01-01 00:00:00             20180.866                          1017.279   \n2017-01-01 00:15:00             20180.866                          1017.284   \n2017-01-01 00:30:00             20180.866                          1017.289   \n2017-01-01 00:45:00             20180.866                          1017.294   \n2017-01-01 01:00:00             20180.866                          1017.299   \n...                                   ...                               ...   \n2019-05-01 21:00:00                   NaN                               NaN   \n2019-05-01 21:15:00                   NaN                               NaN   \n2019-05-01 21:30:00                   NaN                               NaN   \n2019-05-01 21:45:00                   NaN                               NaN   \n2019-05-01 22:00:00                   NaN                               NaN   \n\n                     DE_KN_industrial2_storage_decharge  \\\nutc_timestamp                                             \n2017-01-01 00:00:00                             620.249   \n2017-01-01 00:15:00                             620.249   \n2017-01-01 00:30:00                             620.249   \n2017-01-01 00:45:00                             620.249   \n2017-01-01 01:00:00                             620.249   \n...                                                 ...   \n2019-05-01 21:00:00                                 NaN   \n2019-05-01 21:15:00                                 NaN   \n2019-05-01 21:30:00                                 NaN   \n2019-05-01 21:45:00                                 NaN   \n2019-05-01 22:00:00                                 NaN   \n\n                     DE_KN_industrial3_area_offices  \\\nutc_timestamp                                         \n2017-01-01 00:00:00                       12351.429   \n2017-01-01 00:15:00                       12351.509   \n2017-01-01 00:30:00                       12351.584   \n2017-01-01 00:45:00                       12351.649   \n2017-01-01 01:00:00                       12351.720   \n...                                             ...   \n2019-05-01 21:00:00                             NaN   \n2019-05-01 21:15:00                             NaN   \n2019-05-01 21:30:00                             NaN   \n2019-05-01 21:45:00                             NaN   \n2019-05-01 22:00:00                             NaN   \n\n                     DE_KN_industrial3_area_room_1  \\\nutc_timestamp                                        \n2017-01-01 00:00:00                       4700.859   \n2017-01-01 00:15:00                       4700.889   \n2017-01-01 00:30:00                       4700.918   \n2017-01-01 00:45:00                       4700.944   \n2017-01-01 01:00:00                       4700.969   \n...                                            ...   \n2019-05-01 21:00:00                            NaN   \n2019-05-01 21:15:00                            NaN   \n2019-05-01 21:30:00                            NaN   \n2019-05-01 21:45:00                            NaN   \n2019-05-01 22:00:00                            NaN   \n\n                     DE_KN_industrial3_area_room_2  ...  \\\nutc_timestamp                                       ...   \n2017-01-01 00:00:00                      21517.785  ...   \n2017-01-01 00:15:00                      21517.810  ...   \n2017-01-01 00:30:00                      21517.836  ...   \n2017-01-01 00:45:00                      21517.855  ...   \n2017-01-01 01:00:00                      21517.883  ...   \n...                                            ...  ...   \n2019-05-01 21:00:00                            NaN  ...   \n2019-05-01 21:15:00                            NaN  ...   \n2019-05-01 21:30:00                            NaN  ...   \n2019-05-01 21:45:00                            NaN  ...   \n2019-05-01 22:00:00                            NaN  ...   \n\n                     DE_KN_residential5_refrigerator  \\\nutc_timestamp                                          \n2017-01-01 00:00:00                          429.432   \n2017-01-01 00:15:00                          429.432   \n2017-01-01 00:30:00                          429.445   \n2017-01-01 00:45:00                          429.459   \n2017-01-01 01:00:00                          429.481   \n...                                              ...   \n2019-05-01 21:00:00                              NaN   \n2019-05-01 21:15:00                              NaN   \n2019-05-01 21:30:00                              NaN   \n2019-05-01 21:45:00                              NaN   \n2019-05-01 22:00:00                              NaN   \n\n                     DE_KN_residential5_washing_machine  \\\nutc_timestamp                                             \n2017-01-01 00:00:00                             221.069   \n2017-01-01 00:15:00                             221.069   \n2017-01-01 00:30:00                             221.069   \n2017-01-01 00:45:00                             221.069   \n2017-01-01 01:00:00                             221.069   \n...                                                 ...   \n2019-05-01 21:00:00                                 NaN   \n2019-05-01 21:15:00                                 NaN   \n2019-05-01 21:30:00                                 NaN   \n2019-05-01 21:45:00                                 NaN   \n2019-05-01 22:00:00                                 NaN   \n\n                     DE_KN_residential6_circulation_pump  \\\nutc_timestamp                                              \n2017-01-01 00:00:00                              425.378   \n2017-01-01 00:15:00                              425.392   \n2017-01-01 00:30:00                              425.399   \n2017-01-01 00:45:00                              425.406   \n2017-01-01 01:00:00                              425.413   \n...                                                  ...   \n2019-05-01 21:00:00                                  NaN   \n2019-05-01 21:15:00                                  NaN   \n2019-05-01 21:30:00                                  NaN   \n2019-05-01 21:45:00                                  NaN   \n2019-05-01 22:00:00                                  NaN   \n\n                     DE_KN_residential6_dishwasher  \\\nutc_timestamp                                        \n2017-01-01 00:00:00                         69.508   \n2017-01-01 00:15:00                         69.508   \n2017-01-01 00:30:00                         69.508   \n2017-01-01 00:45:00                         69.508   \n2017-01-01 01:00:00                         69.508   \n...                                            ...   \n2019-05-01 21:00:00                            NaN   \n2019-05-01 21:15:00                            NaN   \n2019-05-01 21:30:00                            NaN   \n2019-05-01 21:45:00                            NaN   \n2019-05-01 22:00:00                            NaN   \n\n                     DE_KN_residential6_freezer  \\\nutc_timestamp                                     \n2017-01-01 00:00:00                     114.811   \n2017-01-01 00:15:00                     114.811   \n2017-01-01 00:30:00                     114.812   \n2017-01-01 00:45:00                     114.813   \n2017-01-01 01:00:00                     114.813   \n...                                         ...   \n2019-05-01 21:00:00                         NaN   \n2019-05-01 21:15:00                         NaN   \n2019-05-01 21:30:00                         NaN   \n2019-05-01 21:45:00                         NaN   \n2019-05-01 22:00:00                         NaN   \n\n                     DE_KN_residential6_grid_export  \\\nutc_timestamp                                         \n2017-01-01 00:00:00                         1534.29   \n2017-01-01 00:15:00                         1534.29   \n2017-01-01 00:30:00                         1534.29   \n2017-01-01 00:45:00                         1534.29   \n2017-01-01 01:00:00                         1534.29   \n...                                             ...   \n2019-05-01 21:00:00                             NaN   \n2019-05-01 21:15:00                             NaN   \n2019-05-01 21:30:00                             NaN   \n2019-05-01 21:45:00                             NaN   \n2019-05-01 22:00:00                             NaN   \n\n                     DE_KN_residential6_grid_import  DE_KN_residential6_pv  \\\nutc_timestamp                                                                \n2017-01-01 00:00:00                        4337.069               9524.441   \n2017-01-01 00:15:00                        4337.154               9524.441   \n2017-01-01 00:30:00                        4337.239               9524.441   \n2017-01-01 00:45:00                        4337.299               9524.441   \n2017-01-01 01:00:00                        4337.354               9524.441   \n...                                             ...                    ...   \n2019-05-01 21:00:00                             NaN                    NaN   \n2019-05-01 21:15:00                             NaN                    NaN   \n2019-05-01 21:30:00                             NaN                    NaN   \n2019-05-01 21:45:00                             NaN                    NaN   \n2019-05-01 22:00:00                             NaN                    NaN   \n\n                     DE_KN_residential6_washing_machine  \\\nutc_timestamp                                             \n2017-01-01 00:00:00                              49.388   \n2017-01-01 00:15:00                              49.388   \n2017-01-01 00:30:00                              49.388   \n2017-01-01 00:45:00                              49.388   \n2017-01-01 01:00:00                              49.388   \n...                                                 ...   \n2019-05-01 21:00:00                                 NaN   \n2019-05-01 21:15:00                                 NaN   \n2019-05-01 21:30:00                                 NaN   \n2019-05-01 21:45:00                                 NaN   \n2019-05-01 22:00:00                                 NaN   \n\n                                  interpolated  \nutc_timestamp                                   \n2017-01-01 00:00:00  DE_KN_public2_grid_import  \n2017-01-01 00:15:00  DE_KN_public2_grid_import  \n2017-01-01 00:30:00  DE_KN_public2_grid_import  \n2017-01-01 00:45:00  DE_KN_public2_grid_import  \n2017-01-01 01:00:00  DE_KN_public2_grid_import  \n...                                        ...  \n2019-05-01 21:00:00                        NaN  \n2019-05-01 21:15:00                        NaN  \n2019-05-01 21:30:00                        NaN  \n2019-05-01 21:45:00                        NaN  \n2019-05-01 22:00:00                        NaN  \n\n[81689 rows x 69 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DE_KN_industrial1_grid_import</th>\n      <th>DE_KN_industrial1_pv_1</th>\n      <th>DE_KN_industrial1_pv_2</th>\n      <th>DE_KN_industrial2_grid_import</th>\n      <th>DE_KN_industrial2_pv</th>\n      <th>DE_KN_industrial2_storage_charge</th>\n      <th>DE_KN_industrial2_storage_decharge</th>\n      <th>DE_KN_industrial3_area_offices</th>\n      <th>DE_KN_industrial3_area_room_1</th>\n      <th>DE_KN_industrial3_area_room_2</th>\n      <th>...</th>\n      <th>DE_KN_residential5_refrigerator</th>\n      <th>DE_KN_residential5_washing_machine</th>\n      <th>DE_KN_residential6_circulation_pump</th>\n      <th>DE_KN_residential6_dishwasher</th>\n      <th>DE_KN_residential6_freezer</th>\n      <th>DE_KN_residential6_grid_export</th>\n      <th>DE_KN_residential6_grid_import</th>\n      <th>DE_KN_residential6_pv</th>\n      <th>DE_KN_residential6_washing_machine</th>\n      <th>interpolated</th>\n    </tr>\n    <tr>\n      <th>utc_timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-01-01 00:00:00</th>\n      <td>261301.539</td>\n      <td>5390.673</td>\n      <td>4253.803</td>\n      <td>10716.058</td>\n      <td>20180.866</td>\n      <td>1017.279</td>\n      <td>620.249</td>\n      <td>12351.429</td>\n      <td>4700.859</td>\n      <td>21517.785</td>\n      <td>...</td>\n      <td>429.432</td>\n      <td>221.069</td>\n      <td>425.378</td>\n      <td>69.508</td>\n      <td>114.811</td>\n      <td>1534.29</td>\n      <td>4337.069</td>\n      <td>9524.441</td>\n      <td>49.388</td>\n      <td>DE_KN_public2_grid_import</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 00:15:00</th>\n      <td>261303.961</td>\n      <td>5390.673</td>\n      <td>4253.803</td>\n      <td>10716.449</td>\n      <td>20180.866</td>\n      <td>1017.284</td>\n      <td>620.249</td>\n      <td>12351.509</td>\n      <td>4700.889</td>\n      <td>21517.810</td>\n      <td>...</td>\n      <td>429.432</td>\n      <td>221.069</td>\n      <td>425.392</td>\n      <td>69.508</td>\n      <td>114.811</td>\n      <td>1534.29</td>\n      <td>4337.154</td>\n      <td>9524.441</td>\n      <td>49.388</td>\n      <td>DE_KN_public2_grid_import</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 00:30:00</th>\n      <td>261305.914</td>\n      <td>5390.673</td>\n      <td>4253.803</td>\n      <td>10716.865</td>\n      <td>20180.866</td>\n      <td>1017.289</td>\n      <td>620.249</td>\n      <td>12351.584</td>\n      <td>4700.918</td>\n      <td>21517.836</td>\n      <td>...</td>\n      <td>429.445</td>\n      <td>221.069</td>\n      <td>425.399</td>\n      <td>69.508</td>\n      <td>114.812</td>\n      <td>1534.29</td>\n      <td>4337.239</td>\n      <td>9524.441</td>\n      <td>49.388</td>\n      <td>DE_KN_public2_grid_import</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 00:45:00</th>\n      <td>261309.914</td>\n      <td>5390.673</td>\n      <td>4253.803</td>\n      <td>10717.264</td>\n      <td>20180.866</td>\n      <td>1017.294</td>\n      <td>620.249</td>\n      <td>12351.649</td>\n      <td>4700.944</td>\n      <td>21517.855</td>\n      <td>...</td>\n      <td>429.459</td>\n      <td>221.069</td>\n      <td>425.406</td>\n      <td>69.508</td>\n      <td>114.813</td>\n      <td>1534.29</td>\n      <td>4337.299</td>\n      <td>9524.441</td>\n      <td>49.388</td>\n      <td>DE_KN_public2_grid_import</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 01:00:00</th>\n      <td>261312.602</td>\n      <td>5390.673</td>\n      <td>4253.803</td>\n      <td>10717.680</td>\n      <td>20180.866</td>\n      <td>1017.299</td>\n      <td>620.249</td>\n      <td>12351.720</td>\n      <td>4700.969</td>\n      <td>21517.883</td>\n      <td>...</td>\n      <td>429.481</td>\n      <td>221.069</td>\n      <td>425.413</td>\n      <td>69.508</td>\n      <td>114.813</td>\n      <td>1534.29</td>\n      <td>4337.354</td>\n      <td>9524.441</td>\n      <td>49.388</td>\n      <td>DE_KN_public2_grid_import</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2019-05-01 21:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-05-01 21:15:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-05-01 21:30:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-05-01 21:45:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-05-01 22:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>81689 rows  69 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# We see that there are rows with NaN, lets clear them out \n",
    "household_data_df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 81689 entries, 2017-01-01 00:00:00 to 2019-05-01 22:00:00\nData columns (total 68 columns):\n #   Column                               Non-Null Count  Dtype  \n---  ------                               --------------  -----  \n 0   DE_KN_industrial1_grid_import        27353 non-null  float64\n 1   DE_KN_industrial1_pv_1               27352 non-null  float64\n 2   DE_KN_industrial1_pv_2               27352 non-null  float64\n 3   DE_KN_industrial2_grid_import        14977 non-null  float64\n 4   DE_KN_industrial2_pv                 6202 non-null   float64\n 5   DE_KN_industrial2_storage_charge     14977 non-null  float64\n 6   DE_KN_industrial2_storage_decharge   14977 non-null  float64\n 7   DE_KN_industrial3_area_offices       14873 non-null  float64\n 8   DE_KN_industrial3_area_room_1        14873 non-null  float64\n 9   DE_KN_industrial3_area_room_2        14873 non-null  float64\n 10  DE_KN_industrial3_area_room_3        14873 non-null  float64\n 11  DE_KN_industrial3_area_room_4        14873 non-null  float64\n 12  DE_KN_industrial3_compressor         14873 non-null  float64\n 13  DE_KN_industrial3_cooling_aggregate  14873 non-null  float64\n 14  DE_KN_industrial3_cooling_pumps      14873 non-null  float64\n 15  DE_KN_industrial3_dishwasher         14873 non-null  float64\n 16  DE_KN_industrial3_ev                 14873 non-null  float64\n 17  DE_KN_industrial3_grid_import        14873 non-null  float64\n 18  DE_KN_industrial3_machine_1          14873 non-null  float64\n 19  DE_KN_industrial3_machine_2          14873 non-null  float64\n 20  DE_KN_industrial3_machine_3          14873 non-null  float64\n 21  DE_KN_industrial3_machine_4          14873 non-null  float64\n 22  DE_KN_industrial3_machine_5          14873 non-null  float64\n 23  DE_KN_industrial3_pv_facade          14873 non-null  float64\n 24  DE_KN_industrial3_pv_roof            14873 non-null  float64\n 25  DE_KN_industrial3_refrigerator       14873 non-null  float64\n 26  DE_KN_industrial3_ventilation        14873 non-null  float64\n 27  DE_KN_public1_grid_import            0 non-null      float64\n 28  DE_KN_public2_grid_import            1570 non-null   float64\n 29  DE_KN_residential1_dishwasher        6791 non-null   float64\n 30  DE_KN_residential1_freezer           6791 non-null   float64\n 31  DE_KN_residential1_grid_import       6813 non-null   float64\n 32  DE_KN_residential1_heat_pump         6813 non-null   float64\n 33  DE_KN_residential1_pv                6813 non-null   float64\n 34  DE_KN_residential1_washing_machine   6791 non-null   float64\n 35  DE_KN_residential2_circulation_pump  17787 non-null  float64\n 36  DE_KN_residential2_dishwasher        15889 non-null  float64\n 37  DE_KN_residential2_freezer           0 non-null      float64\n 38  DE_KN_residential2_grid_import       3035 non-null   float64\n 39  DE_KN_residential2_washing_machine   38288 non-null  float64\n 40  DE_KN_residential3_circulation_pump  60719 non-null  float64\n 41  DE_KN_residential3_dishwasher        44263 non-null  float64\n 42  DE_KN_residential3_freezer           42804 non-null  float64\n 43  DE_KN_residential3_grid_export       60719 non-null  float64\n 44  DE_KN_residential3_grid_import       18113 non-null  float64\n 45  DE_KN_residential3_pv                60719 non-null  float64\n 46  DE_KN_residential3_refrigerator      28957 non-null  float64\n 47  DE_KN_residential3_washing_machine   50061 non-null  float64\n 48  DE_KN_residential4_dishwasher        7726 non-null   float64\n 49  DE_KN_residential4_ev                38397 non-null  float64\n 50  DE_KN_residential4_freezer           38397 non-null  float64\n 51  DE_KN_residential4_grid_export       38397 non-null  float64\n 52  DE_KN_residential4_grid_import       38397 non-null  float64\n 53  DE_KN_residential4_heat_pump         6201 non-null   float64\n 54  DE_KN_residential4_pv                38397 non-null  float64\n 55  DE_KN_residential4_refrigerator      368 non-null    float64\n 56  DE_KN_residential4_washing_machine   6277 non-null   float64\n 57  DE_KN_residential5_dishwasher        52639 non-null  float64\n 58  DE_KN_residential5_grid_import       81689 non-null  float64\n 59  DE_KN_residential5_refrigerator      16732 non-null  float64\n 60  DE_KN_residential5_washing_machine   52639 non-null  float64\n 61  DE_KN_residential6_circulation_pump  44441 non-null  float64\n 62  DE_KN_residential6_dishwasher        14547 non-null  float64\n 63  DE_KN_residential6_freezer           23252 non-null  float64\n 64  DE_KN_residential6_grid_export       44441 non-null  float64\n 65  DE_KN_residential6_grid_import       44441 non-null  float64\n 66  DE_KN_residential6_pv                44441 non-null  float64\n 67  DE_KN_residential6_washing_machine   17440 non-null  float64\ndtypes: float64(68)\nmemory usage: 43.0 MB\n"
    }
   ],
   "source": [
    "# Since the data seems to be agregated, let us calculate actual generation at each time stamp \n",
    "household_data_generation_df = household_data_df.copy(deep=True)\n",
    "household_data_generation_df.drop(columns=\"interpolated\", inplace=True)\n",
    "household_data_generation_df.info(verbose=True) # before calculating generation, we want to ensure that all columns are actual numeric and there is no character data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data_generation_df = household_data_generation_df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate hourly generation data - let us try a few things \n",
    "hour_grouping = household_data_generation_df.index.hour\n",
    "hourly_grouped_df = household_data_generation_df.groupby(hour_grouping).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above result might catch you off guard, but if you think about it, you are only grouping by hour, in order get hourly data for our data set, we want to group by all the unique attributes of time upto hour, so - year, month, day, hour\n",
    "hourly_grouping_sel = [(household_data_generation_df.index.year), \n",
    "                       (household_data_generation_df.index.month),\n",
    "                       (household_data_generation_df.index.day),\n",
    "                       (household_data_generation_df.index.hour)]\n",
    "hourly_data_total_df = household_data_generation_df.groupby(hourly_grouping_sel).sum() \n",
    "hourly_data_stats_df = household_data_generation_df.groupby(hourly_grouping_sel).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}